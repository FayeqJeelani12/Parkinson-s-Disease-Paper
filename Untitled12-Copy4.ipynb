{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50780</td>\n",
       "      <td>0.78744</td>\n",
       "      <td>0.65451</td>\n",
       "      <td>226</td>\n",
       "      <td>221</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.00783</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8025</td>\n",
       "      <td>5.0734</td>\n",
       "      <td>7.0166</td>\n",
       "      <td>5.9966</td>\n",
       "      <td>5.2065</td>\n",
       "      <td>7.4246</td>\n",
       "      <td>3.4153</td>\n",
       "      <td>3.5046</td>\n",
       "      <td>3.2250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76095</td>\n",
       "      <td>0.62145</td>\n",
       "      <td>0.54543</td>\n",
       "      <td>322</td>\n",
       "      <td>321</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.00222</td>\n",
       "      <td>...</td>\n",
       "      <td>117.2678</td>\n",
       "      <td>75.3156</td>\n",
       "      <td>32.0478</td>\n",
       "      <td>7.7060</td>\n",
       "      <td>3.1060</td>\n",
       "      <td>4.6206</td>\n",
       "      <td>12.8353</td>\n",
       "      <td>13.8300</td>\n",
       "      <td>7.7693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83671</td>\n",
       "      <td>0.62079</td>\n",
       "      <td>0.51179</td>\n",
       "      <td>318</td>\n",
       "      <td>317</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8564</td>\n",
       "      <td>11.8909</td>\n",
       "      <td>7.2891</td>\n",
       "      <td>4.3682</td>\n",
       "      <td>3.6443</td>\n",
       "      <td>5.9610</td>\n",
       "      <td>11.7552</td>\n",
       "      <td>18.0927</td>\n",
       "      <td>5.0448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80826</td>\n",
       "      <td>0.61766</td>\n",
       "      <td>0.50447</td>\n",
       "      <td>318</td>\n",
       "      <td>317</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2640</td>\n",
       "      <td>6.3993</td>\n",
       "      <td>4.4165</td>\n",
       "      <td>4.2662</td>\n",
       "      <td>3.6357</td>\n",
       "      <td>3.7346</td>\n",
       "      <td>2.9394</td>\n",
       "      <td>3.6216</td>\n",
       "      <td>3.8430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85302</td>\n",
       "      <td>0.62247</td>\n",
       "      <td>0.54855</td>\n",
       "      <td>493</td>\n",
       "      <td>492</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6796</td>\n",
       "      <td>2.0474</td>\n",
       "      <td>2.8117</td>\n",
       "      <td>3.5070</td>\n",
       "      <td>3.2727</td>\n",
       "      <td>3.8415</td>\n",
       "      <td>3.9439</td>\n",
       "      <td>5.8807</td>\n",
       "      <td>38.7211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 755 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0   0       1  0.85247  0.71826  0.57227        240               239   \n",
       "1   0       1  0.76686  0.69481  0.53966        234               233   \n",
       "2   0       1  0.85083  0.67604  0.58982        232               231   \n",
       "3   1       0  0.41121  0.79672  0.59257        178               177   \n",
       "4   1       0  0.32790  0.79782  0.53028        236               235   \n",
       "5   1       0  0.50780  0.78744  0.65451        226               221   \n",
       "6   2       1  0.76095  0.62145  0.54543        322               321   \n",
       "7   2       1  0.83671  0.62079  0.51179        318               317   \n",
       "8   2       1  0.80826  0.61766  0.50447        318               317   \n",
       "9   3       0  0.85302  0.62247  0.54855        493               492   \n",
       "\n",
       "   meanPeriodPulses  stdDevPeriodPulses  locPctJitter  ...    \\\n",
       "0          0.008064            0.000087       0.00218  ...     \n",
       "1          0.008258            0.000073       0.00195  ...     \n",
       "2          0.008340            0.000060       0.00176  ...     \n",
       "3          0.010858            0.000183       0.00419  ...     \n",
       "4          0.008162            0.002669       0.00535  ...     \n",
       "5          0.007631            0.002696       0.00783  ...     \n",
       "6          0.005991            0.000107       0.00222  ...     \n",
       "7          0.006074            0.000136       0.00282  ...     \n",
       "8          0.006057            0.000069       0.00161  ...     \n",
       "9          0.003910            0.000040       0.00075  ...     \n",
       "\n",
       "   tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                     1.5620                     2.6445   \n",
       "1                     1.5589                     3.6107   \n",
       "2                     1.5643                     2.3308   \n",
       "3                     3.7805                     3.5664   \n",
       "4                     6.1727                     5.8416   \n",
       "5                     4.8025                     5.0734   \n",
       "6                   117.2678                    75.3156   \n",
       "7                     3.8564                    11.8909   \n",
       "8                     2.2640                     6.3993   \n",
       "9                     1.6796                     2.0474   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                     3.8686                     4.2105   \n",
       "1                    23.5155                    14.1962   \n",
       "2                     9.4959                    10.7458   \n",
       "3                     5.2558                    14.0403   \n",
       "4                     6.0805                     5.7621   \n",
       "5                     7.0166                     5.9966   \n",
       "6                    32.0478                     7.7060   \n",
       "7                     7.2891                     4.3682   \n",
       "8                     4.4165                     4.2662   \n",
       "9                     2.8117                     3.5070   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                     5.1221                     4.4625   \n",
       "1                    11.0261                     9.5082   \n",
       "2                    11.0177                     4.8066   \n",
       "3                     4.2235                     4.6857   \n",
       "4                     7.7817                    11.6891   \n",
       "5                     5.2065                     7.4246   \n",
       "6                     3.1060                     4.6206   \n",
       "7                     3.6443                     5.9610   \n",
       "8                     3.6357                     3.7346   \n",
       "9                     3.2727                     3.8415   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                     2.6202                     3.0004   \n",
       "1                     6.5245                     6.3431   \n",
       "2                     2.9199                     3.1495   \n",
       "3                     4.8460                     6.2650   \n",
       "4                     8.2103                     5.0559   \n",
       "5                     3.4153                     3.5046   \n",
       "6                    12.8353                    13.8300   \n",
       "7                    11.7552                    18.0927   \n",
       "8                     2.9394                     3.6216   \n",
       "9                     3.9439                     5.8807   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_36  class  \n",
       "0                    18.9405      1  \n",
       "1                    45.1780      1  \n",
       "2                     4.7666      1  \n",
       "3                     4.0603      1  \n",
       "4                     6.1164      1  \n",
       "5                     3.2250      1  \n",
       "6                     7.7693      1  \n",
       "7                     5.0448      1  \n",
       "8                     3.8430      1  \n",
       "9                    38.7211      1  \n",
       "\n",
       "[10 rows x 755 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset \n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "df = pd.read_csv(\"pd_speech_features_1.csv\")\n",
    "df.to_csv(r'C:\\Data\\pd.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 755)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>locShimmer</th>\n",
       "      <th>locDbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>meanAutoCorrHarmonicity</th>\n",
       "      <th>meanNoiseToHarmHarmonicity</th>\n",
       "      <th>meanHarmToNoiseHarmonicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.03011</td>\n",
       "      <td>0.03496</td>\n",
       "      <td>0.04828</td>\n",
       "      <td>0.09034</td>\n",
       "      <td>0.970805</td>\n",
       "      <td>0.036223</td>\n",
       "      <td>18.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.05516</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.02320</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.06195</td>\n",
       "      <td>0.06961</td>\n",
       "      <td>0.984322</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>21.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>0.09902</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.05094</td>\n",
       "      <td>0.06497</td>\n",
       "      <td>0.07772</td>\n",
       "      <td>0.15282</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.026313</td>\n",
       "      <td>17.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00446</td>\n",
       "      <td>0.05451</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.02395</td>\n",
       "      <td>0.02857</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.07185</td>\n",
       "      <td>0.968343</td>\n",
       "      <td>0.042003</td>\n",
       "      <td>19.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.00166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00499</td>\n",
       "      <td>0.05610</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.02909</td>\n",
       "      <td>0.03327</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>0.08728</td>\n",
       "      <td>0.975754</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>19.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.50780</td>\n",
       "      <td>0.78744</td>\n",
       "      <td>0.65451</td>\n",
       "      <td>226</td>\n",
       "      <td>221</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.00783</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00697</td>\n",
       "      <td>0.07752</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.03805</td>\n",
       "      <td>0.04767</td>\n",
       "      <td>0.06451</td>\n",
       "      <td>0.11415</td>\n",
       "      <td>0.906720</td>\n",
       "      <td>0.137088</td>\n",
       "      <td>14.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.76095</td>\n",
       "      <td>0.62145</td>\n",
       "      <td>0.54543</td>\n",
       "      <td>322</td>\n",
       "      <td>321</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.00222</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00108</td>\n",
       "      <td>0.03203</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.01971</td>\n",
       "      <td>0.03274</td>\n",
       "      <td>0.04650</td>\n",
       "      <td>0.984564</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>18.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.83671</td>\n",
       "      <td>0.62079</td>\n",
       "      <td>0.51179</td>\n",
       "      <td>318</td>\n",
       "      <td>317</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.06300</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.02949</td>\n",
       "      <td>0.04091</td>\n",
       "      <td>0.06445</td>\n",
       "      <td>0.08848</td>\n",
       "      <td>0.987625</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>20.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.80826</td>\n",
       "      <td>0.61766</td>\n",
       "      <td>0.50447</td>\n",
       "      <td>318</td>\n",
       "      <td>317</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>0.02783</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.01376</td>\n",
       "      <td>0.01760</td>\n",
       "      <td>0.02698</td>\n",
       "      <td>0.04129</td>\n",
       "      <td>0.992393</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>22.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.85302</td>\n",
       "      <td>0.62247</td>\n",
       "      <td>0.54855</td>\n",
       "      <td>493</td>\n",
       "      <td>492</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.02692</td>\n",
       "      <td>0.03344</td>\n",
       "      <td>0.05630</td>\n",
       "      <td>0.08077</td>\n",
       "      <td>0.994504</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>23.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PPE      DFA     RPDE  numPulses  numPeriodsPulses  meanPeriodPulses  \\\n",
       "0  0.85247  0.71826  0.57227        240               239          0.008064   \n",
       "1  0.76686  0.69481  0.53966        234               233          0.008258   \n",
       "2  0.85083  0.67604  0.58982        232               231          0.008340   \n",
       "3  0.41121  0.79672  0.59257        178               177          0.010858   \n",
       "4  0.32790  0.79782  0.53028        236               235          0.008162   \n",
       "5  0.50780  0.78744  0.65451        226               221          0.007631   \n",
       "6  0.76095  0.62145  0.54543        322               321          0.005991   \n",
       "7  0.83671  0.62079  0.51179        318               317          0.006074   \n",
       "8  0.80826  0.61766  0.50447        318               317          0.006057   \n",
       "9  0.85302  0.62247  0.54855        493               492          0.003910   \n",
       "\n",
       "   stdDevPeriodPulses  locPctJitter  locAbsJitter  rapJitter  \\\n",
       "0            0.000087       0.00218      0.000018    0.00067   \n",
       "1            0.000073       0.00195      0.000016    0.00052   \n",
       "2            0.000060       0.00176      0.000015    0.00057   \n",
       "3            0.000183       0.00419      0.000046    0.00149   \n",
       "4            0.002669       0.00535      0.000044    0.00166   \n",
       "5            0.002696       0.00783      0.000060    0.00232   \n",
       "6            0.000107       0.00222      0.000013    0.00036   \n",
       "7            0.000136       0.00282      0.000017    0.00034   \n",
       "8            0.000069       0.00161      0.000010    0.00027   \n",
       "9            0.000040       0.00075      0.000003    0.00009   \n",
       "\n",
       "              ...              ddpJitter  locShimmer  locDbShimmer  \\\n",
       "0             ...                0.00200     0.05883         0.517   \n",
       "1             ...                0.00157     0.05516         0.502   \n",
       "2             ...                0.00171     0.09902         0.897   \n",
       "3             ...                0.00446     0.05451         0.527   \n",
       "4             ...                0.00499     0.05610         0.497   \n",
       "5             ...                0.00697     0.07752         0.678   \n",
       "6             ...                0.00108     0.03203         0.280   \n",
       "7             ...                0.00103     0.06300         0.539   \n",
       "8             ...                0.00081     0.02783         0.244   \n",
       "9             ...                0.00027     0.05670         0.512   \n",
       "\n",
       "   apq3Shimmer  apq5Shimmer  apq11Shimmer  ddaShimmer  \\\n",
       "0      0.03011      0.03496       0.04828     0.09034   \n",
       "1      0.02320      0.03675       0.06195     0.06961   \n",
       "2      0.05094      0.06497       0.07772     0.15282   \n",
       "3      0.02395      0.02857       0.04462     0.07185   \n",
       "4      0.02909      0.03327       0.05278     0.08728   \n",
       "5      0.03805      0.04767       0.06451     0.11415   \n",
       "6      0.01550      0.01971       0.03274     0.04650   \n",
       "7      0.02949      0.04091       0.06445     0.08848   \n",
       "8      0.01376      0.01760       0.02698     0.04129   \n",
       "9      0.02692      0.03344       0.05630     0.08077   \n",
       "\n",
       "   meanAutoCorrHarmonicity  meanNoiseToHarmHarmonicity  \\\n",
       "0                 0.970805                    0.036223   \n",
       "1                 0.984322                    0.017974   \n",
       "2                 0.974846                    0.026313   \n",
       "3                 0.968343                    0.042003   \n",
       "4                 0.975754                    0.027139   \n",
       "5                 0.906720                    0.137088   \n",
       "6                 0.984564                    0.015745   \n",
       "7                 0.987625                    0.012621   \n",
       "8                 0.992393                    0.007690   \n",
       "9                 0.994504                    0.005538   \n",
       "\n",
       "   meanHarmToNoiseHarmonicity  \n",
       "0                      18.995  \n",
       "1                      21.497  \n",
       "2                      17.651  \n",
       "3                      19.865  \n",
       "4                      19.557  \n",
       "5                      14.676  \n",
       "6                      18.670  \n",
       "7                      20.302  \n",
       "8                      22.376  \n",
       "9                      23.426  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implementing algorithm \n",
    "#model 1 \n",
    "df1 = df.iloc[:,2:23]\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=df['class']\n",
    "#X = df.iloc[:,2:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,2:23]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation,this code divides the data into splits, here we take 5 splits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy'\n",
    "models = []\n",
    "\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=10, random_state=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.775580 (0.014014)\n",
      "CART: 0.729421 (0.034749)\n",
      "NB: 0.641990 (0.068983)\n",
      "RF: 0.749296 (0.042523)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=skf, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DT', '             precision    recall  f1-score   support\\n\\n          0       0.25      0.26      0.26        38\\n          1       0.75      0.73      0.74       112\\n\\navg / total       0.62      0.61      0.62       150\\n'), ('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.39      0.37      0.38        38\\n          1       0.79      0.80      0.80       112\\n\\navg / total       0.69      0.69      0.69       150\\n'), ('LR', '             precision    recall  f1-score   support\\n\\n          0       0.20      0.03      0.05        38\\n          1       0.74      0.96      0.84       112\\n\\navg / total       0.61      0.73      0.64       150\\n'), ('DT', '             precision    recall  f1-score   support\\n\\n          0       0.25      0.26      0.26        38\\n          1       0.75      0.73      0.74       112\\n\\navg / total       0.62      0.61      0.62       150\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(classificationreport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DT', 0.6133333333333333)\n",
      "('Random Forest', 0.6933333333333334)\n",
      "('LR', 0.7266666666666667)\n",
      "('DT', 0.6133333333333333)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6266666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for intesity features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minIntensity</th>\n",
       "      <th>maxIntensity</th>\n",
       "      <th>meanIntensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.997496</td>\n",
       "      <td>76.088046</td>\n",
       "      <td>72.465512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.415903</td>\n",
       "      <td>73.046374</td>\n",
       "      <td>71.528945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.661706</td>\n",
       "      <td>71.633549</td>\n",
       "      <td>68.086583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.306989</td>\n",
       "      <td>81.000749</td>\n",
       "      <td>79.190593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.645686</td>\n",
       "      <td>80.937258</td>\n",
       "      <td>79.183495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74.084566</td>\n",
       "      <td>80.614592</td>\n",
       "      <td>78.732944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75.059718</td>\n",
       "      <td>78.381263</td>\n",
       "      <td>77.077102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75.097654</td>\n",
       "      <td>78.465346</td>\n",
       "      <td>76.841929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75.189839</td>\n",
       "      <td>78.217873</td>\n",
       "      <td>76.974052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71.256618</td>\n",
       "      <td>80.032378</td>\n",
       "      <td>76.086098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minIntensity  maxIntensity  meanIntensity\n",
       "0     69.997496     76.088046      72.465512\n",
       "1     67.415903     73.046374      71.528945\n",
       "2     62.661706     71.633549      68.086583\n",
       "3     76.306989     81.000749      79.190593\n",
       "4     76.645686     80.937258      79.183495\n",
       "5     74.084566     80.614592      78.732944\n",
       "6     75.059718     78.381263      77.077102\n",
       "7     75.097654     78.465346      76.841929\n",
       "8     75.189839     78.217873      76.974052\n",
       "9     71.256618     80.032378      76.086098"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,23:26]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,23:26]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.36      0.79      0.50        38\\n          1       0.88      0.53      0.66       112\\n\\navg / total       0.75      0.59      0.62       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.34      0.37      0.35        38\\n          1       0.78      0.76      0.77       112\\n\\navg / total       0.67      0.66      0.66       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.37      0.42      0.40        38\\n          1       0.79      0.76      0.78       112\\n\\navg / total       0.69      0.67      0.68       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.37      0.42      0.40        38\\n          1       0.79      0.76      0.78       112\\n\\navg / total       0.69      0.67      0.68       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.5933333333333334)\n",
      "('Random Forest', 0.66)\n",
      "('DT', 0.6733333333333333)\n",
      "('DT', 0.6733333333333333)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minIntensity</th>\n",
       "      <th>maxIntensity</th>\n",
       "      <th>meanIntensity</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.997496</td>\n",
       "      <td>76.088046</td>\n",
       "      <td>72.465512</td>\n",
       "      <td>539.342735</td>\n",
       "      <td>1031.849040</td>\n",
       "      <td>2447.162183</td>\n",
       "      <td>3655.054806</td>\n",
       "      <td>101.092218</td>\n",
       "      <td>83.147440</td>\n",
       "      <td>255.214830</td>\n",
       "      <td>396.643631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.415903</td>\n",
       "      <td>73.046374</td>\n",
       "      <td>71.528945</td>\n",
       "      <td>564.363614</td>\n",
       "      <td>1016.367294</td>\n",
       "      <td>2383.565201</td>\n",
       "      <td>3498.681572</td>\n",
       "      <td>58.465428</td>\n",
       "      <td>86.487292</td>\n",
       "      <td>248.357127</td>\n",
       "      <td>218.229722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.661706</td>\n",
       "      <td>71.633549</td>\n",
       "      <td>68.086583</td>\n",
       "      <td>548.444604</td>\n",
       "      <td>1032.406341</td>\n",
       "      <td>2357.826954</td>\n",
       "      <td>3678.128717</td>\n",
       "      <td>160.387771</td>\n",
       "      <td>54.685168</td>\n",
       "      <td>151.694847</td>\n",
       "      <td>84.240339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.306989</td>\n",
       "      <td>81.000749</td>\n",
       "      <td>79.190593</td>\n",
       "      <td>819.529588</td>\n",
       "      <td>1201.813897</td>\n",
       "      <td>3154.035654</td>\n",
       "      <td>4122.163933</td>\n",
       "      <td>238.667052</td>\n",
       "      <td>191.984916</td>\n",
       "      <td>573.752909</td>\n",
       "      <td>526.147599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.645686</td>\n",
       "      <td>80.937258</td>\n",
       "      <td>79.183495</td>\n",
       "      <td>846.796144</td>\n",
       "      <td>1215.346469</td>\n",
       "      <td>3201.513132</td>\n",
       "      <td>4085.456839</td>\n",
       "      <td>402.216738</td>\n",
       "      <td>210.061394</td>\n",
       "      <td>203.637106</td>\n",
       "      <td>384.611697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74.084566</td>\n",
       "      <td>80.614592</td>\n",
       "      <td>78.732944</td>\n",
       "      <td>832.437252</td>\n",
       "      <td>1193.223486</td>\n",
       "      <td>3177.289013</td>\n",
       "      <td>3791.213621</td>\n",
       "      <td>238.834750</td>\n",
       "      <td>222.710316</td>\n",
       "      <td>190.203574</td>\n",
       "      <td>663.190792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75.059718</td>\n",
       "      <td>78.381263</td>\n",
       "      <td>77.077102</td>\n",
       "      <td>710.861860</td>\n",
       "      <td>1165.801015</td>\n",
       "      <td>3049.357722</td>\n",
       "      <td>4211.727619</td>\n",
       "      <td>102.075203</td>\n",
       "      <td>69.614851</td>\n",
       "      <td>153.366839</td>\n",
       "      <td>430.029892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75.097654</td>\n",
       "      <td>78.465346</td>\n",
       "      <td>76.841929</td>\n",
       "      <td>668.306515</td>\n",
       "      <td>1165.769341</td>\n",
       "      <td>2954.648489</td>\n",
       "      <td>3560.280084</td>\n",
       "      <td>128.235259</td>\n",
       "      <td>91.930442</td>\n",
       "      <td>238.244533</td>\n",
       "      <td>959.282262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75.189839</td>\n",
       "      <td>78.217873</td>\n",
       "      <td>76.974052</td>\n",
       "      <td>670.639344</td>\n",
       "      <td>1127.450888</td>\n",
       "      <td>2933.208331</td>\n",
       "      <td>3833.298076</td>\n",
       "      <td>137.800940</td>\n",
       "      <td>94.982679</td>\n",
       "      <td>114.811461</td>\n",
       "      <td>160.090034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71.256618</td>\n",
       "      <td>80.032378</td>\n",
       "      <td>76.086098</td>\n",
       "      <td>707.815533</td>\n",
       "      <td>1285.508031</td>\n",
       "      <td>2020.531841</td>\n",
       "      <td>3238.976356</td>\n",
       "      <td>175.856887</td>\n",
       "      <td>103.764094</td>\n",
       "      <td>2695.853491</td>\n",
       "      <td>250.985143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minIntensity  maxIntensity  meanIntensity          f1           f2  \\\n",
       "0     69.997496     76.088046      72.465512  539.342735  1031.849040   \n",
       "1     67.415903     73.046374      71.528945  564.363614  1016.367294   \n",
       "2     62.661706     71.633549      68.086583  548.444604  1032.406341   \n",
       "3     76.306989     81.000749      79.190593  819.529588  1201.813897   \n",
       "4     76.645686     80.937258      79.183495  846.796144  1215.346469   \n",
       "5     74.084566     80.614592      78.732944  832.437252  1193.223486   \n",
       "6     75.059718     78.381263      77.077102  710.861860  1165.801015   \n",
       "7     75.097654     78.465346      76.841929  668.306515  1165.769341   \n",
       "8     75.189839     78.217873      76.974052  670.639344  1127.450888   \n",
       "9     71.256618     80.032378      76.086098  707.815533  1285.508031   \n",
       "\n",
       "            f3           f4          b1          b2           b3          b4  \n",
       "0  2447.162183  3655.054806  101.092218   83.147440   255.214830  396.643631  \n",
       "1  2383.565201  3498.681572   58.465428   86.487292   248.357127  218.229722  \n",
       "2  2357.826954  3678.128717  160.387771   54.685168   151.694847   84.240339  \n",
       "3  3154.035654  4122.163933  238.667052  191.984916   573.752909  526.147599  \n",
       "4  3201.513132  4085.456839  402.216738  210.061394   203.637106  384.611697  \n",
       "5  3177.289013  3791.213621  238.834750  222.710316   190.203574  663.190792  \n",
       "6  3049.357722  4211.727619  102.075203   69.614851   153.366839  430.029892  \n",
       "7  2954.648489  3560.280084  128.235259   91.930442   238.244533  959.282262  \n",
       "8  2933.208331  3833.298076  137.800940   94.982679   114.811461  160.090034  \n",
       "9  2020.531841  3238.976356  175.856887  103.764094  2695.853491  250.985143  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,23:34]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,23:34]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7266666666666667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.6733333333333333)\n",
      "('Random Forest', 0.76)\n",
      "('LR', 0.7333333333333333)\n",
      "('DT', 0.7133333333333334)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.41      0.68      0.51        38\\n          1       0.86      0.67      0.75       112\\n\\navg / total       0.75      0.67      0.69       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.53      0.42      0.47        38\\n          1       0.82      0.88      0.84       112\\n\\navg / total       0.74      0.76      0.75       150\\n')\n",
      "('LR', '             precision    recall  f1-score   support\\n\\n          0       0.47      0.45      0.46        38\\n          1       0.82      0.83      0.82       112\\n\\navg / total       0.73      0.73      0.73       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.43      0.39      0.41        38\\n          1       0.80      0.82      0.81       112\\n\\navg / total       0.71      0.71      0.71       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GQ_prc5_95</th>\n",
       "      <th>GQ_std_cycle_open</th>\n",
       "      <th>GQ_std_cycle_closed</th>\n",
       "      <th>GNE_mean</th>\n",
       "      <th>GNE_std</th>\n",
       "      <th>GNE_SNR_TKEO</th>\n",
       "      <th>GNE_SNR_SEO</th>\n",
       "      <th>GNE_NSR_TKEO</th>\n",
       "      <th>GNE_NSR_SEO</th>\n",
       "      <th>VFER_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>VFER_SNR_TKEO</th>\n",
       "      <th>VFER_SNR_SEO</th>\n",
       "      <th>VFER_NSR_TKEO</th>\n",
       "      <th>VFER_NSR_SEO</th>\n",
       "      <th>IMF_SNR_SEO</th>\n",
       "      <th>IMF_SNR_TKEO</th>\n",
       "      <th>IMF_SNR_entropy</th>\n",
       "      <th>IMF_NSR_SEO</th>\n",
       "      <th>IMF_NSR_TKEO</th>\n",
       "      <th>IMF_NSR_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.77778</td>\n",
       "      <td>11.7245</td>\n",
       "      <td>2.8277</td>\n",
       "      <td>1.17300</td>\n",
       "      <td>0.265120</td>\n",
       "      <td>0.083127</td>\n",
       "      <td>1200445.612</td>\n",
       "      <td>1.5347</td>\n",
       "      <td>3.0152</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>...</td>\n",
       "      <td>209.6062</td>\n",
       "      <td>455.9654</td>\n",
       "      <td>1.2825</td>\n",
       "      <td>1.3305</td>\n",
       "      <td>51.6843</td>\n",
       "      <td>5.77840</td>\n",
       "      <td>23.2610</td>\n",
       "      <td>0.26850</td>\n",
       "      <td>5.8573</td>\n",
       "      <td>0.20023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.81250</td>\n",
       "      <td>13.8284</td>\n",
       "      <td>2.8908</td>\n",
       "      <td>1.02210</td>\n",
       "      <td>0.220040</td>\n",
       "      <td>0.127410</td>\n",
       "      <td>1298455.445</td>\n",
       "      <td>1.6029</td>\n",
       "      <td>3.0600</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>...</td>\n",
       "      <td>243.0816</td>\n",
       "      <td>379.8429</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>1.3177</td>\n",
       "      <td>24.0230</td>\n",
       "      <td>6.79160</td>\n",
       "      <td>21.8851</td>\n",
       "      <td>0.26839</td>\n",
       "      <td>6.2366</td>\n",
       "      <td>0.20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.81818</td>\n",
       "      <td>26.9273</td>\n",
       "      <td>2.6975</td>\n",
       "      <td>0.84951</td>\n",
       "      <td>0.157560</td>\n",
       "      <td>0.116890</td>\n",
       "      <td>1272869.841</td>\n",
       "      <td>1.6223</td>\n",
       "      <td>3.0309</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>...</td>\n",
       "      <td>238.5976</td>\n",
       "      <td>386.4739</td>\n",
       "      <td>1.2614</td>\n",
       "      <td>1.3078</td>\n",
       "      <td>60.0458</td>\n",
       "      <td>11.33760</td>\n",
       "      <td>41.7310</td>\n",
       "      <td>0.23034</td>\n",
       "      <td>4.8081</td>\n",
       "      <td>0.17296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98548</td>\n",
       "      <td>139.5744</td>\n",
       "      <td>1.6961</td>\n",
       "      <td>0.83405</td>\n",
       "      <td>0.172950</td>\n",
       "      <td>0.147370</td>\n",
       "      <td>1932289.206</td>\n",
       "      <td>1.6717</td>\n",
       "      <td>3.0293</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0921</td>\n",
       "      <td>161.1054</td>\n",
       "      <td>1.2369</td>\n",
       "      <td>1.2986</td>\n",
       "      <td>83.6201</td>\n",
       "      <td>5.07840</td>\n",
       "      <td>35.8179</td>\n",
       "      <td>0.23529</td>\n",
       "      <td>4.2629</td>\n",
       "      <td>0.17510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.97847</td>\n",
       "      <td>102.0549</td>\n",
       "      <td>15.4045</td>\n",
       "      <td>0.83556</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.151990</td>\n",
       "      <td>1861807.802</td>\n",
       "      <td>1.6781</td>\n",
       "      <td>3.0362</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>...</td>\n",
       "      <td>53.6764</td>\n",
       "      <td>164.4029</td>\n",
       "      <td>1.2360</td>\n",
       "      <td>1.3156</td>\n",
       "      <td>102.9371</td>\n",
       "      <td>5.60220</td>\n",
       "      <td>31.5211</td>\n",
       "      <td>0.30603</td>\n",
       "      <td>6.3500</td>\n",
       "      <td>0.21877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.73122</td>\n",
       "      <td>104.2897</td>\n",
       "      <td>37.5602</td>\n",
       "      <td>0.85470</td>\n",
       "      <td>0.227640</td>\n",
       "      <td>0.143890</td>\n",
       "      <td>1761740.466</td>\n",
       "      <td>1.6678</td>\n",
       "      <td>3.0297</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1600</td>\n",
       "      <td>100.1507</td>\n",
       "      <td>1.2139</td>\n",
       "      <td>1.2640</td>\n",
       "      <td>59.8518</td>\n",
       "      <td>4.80770</td>\n",
       "      <td>26.7902</td>\n",
       "      <td>0.27327</td>\n",
       "      <td>6.1189</td>\n",
       "      <td>0.19882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>12.0759</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.87505</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.147770</td>\n",
       "      <td>1408372.118</td>\n",
       "      <td>1.6544</td>\n",
       "      <td>3.0247</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>...</td>\n",
       "      <td>68.4688</td>\n",
       "      <td>224.4223</td>\n",
       "      <td>1.2344</td>\n",
       "      <td>1.3039</td>\n",
       "      <td>24.2781</td>\n",
       "      <td>2.46830</td>\n",
       "      <td>11.1578</td>\n",
       "      <td>0.13043</td>\n",
       "      <td>6.3795</td>\n",
       "      <td>0.13555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.1775</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.83430</td>\n",
       "      <td>0.077763</td>\n",
       "      <td>0.161890</td>\n",
       "      <td>1473379.510</td>\n",
       "      <td>1.6531</td>\n",
       "      <td>3.0061</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>...</td>\n",
       "      <td>50.2918</td>\n",
       "      <td>348.4967</td>\n",
       "      <td>1.2397</td>\n",
       "      <td>1.3607</td>\n",
       "      <td>2.6477</td>\n",
       "      <td>0.66830</td>\n",
       "      <td>2.7385</td>\n",
       "      <td>0.12413</td>\n",
       "      <td>20.5870</td>\n",
       "      <td>0.13759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.9031</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.79163</td>\n",
       "      <td>0.062666</td>\n",
       "      <td>0.156350</td>\n",
       "      <td>1434452.951</td>\n",
       "      <td>1.6569</td>\n",
       "      <td>3.0201</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>...</td>\n",
       "      <td>34.5887</td>\n",
       "      <td>603.1786</td>\n",
       "      <td>1.2292</td>\n",
       "      <td>1.3735</td>\n",
       "      <td>14.4723</td>\n",
       "      <td>1.50850</td>\n",
       "      <td>7.0518</td>\n",
       "      <td>0.12068</td>\n",
       "      <td>5.8035</td>\n",
       "      <td>0.13288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.8523</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.53530</td>\n",
       "      <td>0.243820</td>\n",
       "      <td>0.080905</td>\n",
       "      <td>1241110.325</td>\n",
       "      <td>1.5117</td>\n",
       "      <td>3.0698</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>...</td>\n",
       "      <td>85.4586</td>\n",
       "      <td>344.4541</td>\n",
       "      <td>1.2932</td>\n",
       "      <td>1.3603</td>\n",
       "      <td>0.5149</td>\n",
       "      <td>0.14478</td>\n",
       "      <td>1.0844</td>\n",
       "      <td>0.13773</td>\n",
       "      <td>24.4549</td>\n",
       "      <td>0.13205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GQ_prc5_95  GQ_std_cycle_open  GQ_std_cycle_closed  GNE_mean   GNE_std  \\\n",
       "0     0.77778            11.7245               2.8277   1.17300  0.265120   \n",
       "1     0.81250            13.8284               2.8908   1.02210  0.220040   \n",
       "2     0.81818            26.9273               2.6975   0.84951  0.157560   \n",
       "3     0.98548           139.5744               1.6961   0.83405  0.172950   \n",
       "4     0.97847           102.0549              15.4045   0.83556  0.162100   \n",
       "5     0.73122           104.2897              37.5602   0.85470  0.227640   \n",
       "6     1.00000            12.0759               0.0000   0.87505  0.099406   \n",
       "7     1.00000             9.1775               0.0000   0.83430  0.077763   \n",
       "8     1.00000             5.9031               0.0000   0.79163  0.062666   \n",
       "9     1.00000             4.8523               0.0000   1.53530  0.243820   \n",
       "\n",
       "   GNE_SNR_TKEO  GNE_SNR_SEO  GNE_NSR_TKEO  GNE_NSR_SEO  VFER_mean  \\\n",
       "0      0.083127  1200445.612        1.5347       3.0152   0.000463   \n",
       "1      0.127410  1298455.445        1.6029       3.0600   0.000615   \n",
       "2      0.116890  1272869.841        1.6223       3.0309   0.000360   \n",
       "3      0.147370  1932289.206        1.6717       3.0293   0.000196   \n",
       "4      0.151990  1861807.802        1.6781       3.0362   0.000281   \n",
       "5      0.143890  1761740.466        1.6678       3.0297   0.000282   \n",
       "6      0.147770  1408372.118        1.6544       3.0247   0.001706   \n",
       "7      0.161890  1473379.510        1.6531       3.0061   0.001773   \n",
       "8      0.156350  1434452.951        1.6569       3.0201   0.001634   \n",
       "9      0.080905  1241110.325        1.5117       3.0698   0.003112   \n",
       "\n",
       "        ...         VFER_SNR_TKEO  VFER_SNR_SEO  VFER_NSR_TKEO  VFER_NSR_SEO  \\\n",
       "0       ...              209.6062      455.9654         1.2825        1.3305   \n",
       "1       ...              243.0816      379.8429         1.3063        1.3177   \n",
       "2       ...              238.5976      386.4739         1.2614        1.3078   \n",
       "3       ...               79.0921      161.1054         1.2369        1.2986   \n",
       "4       ...               53.6764      164.4029         1.2360        1.3156   \n",
       "5       ...               48.1600      100.1507         1.2139        1.2640   \n",
       "6       ...               68.4688      224.4223         1.2344        1.3039   \n",
       "7       ...               50.2918      348.4967         1.2397        1.3607   \n",
       "8       ...               34.5887      603.1786         1.2292        1.3735   \n",
       "9       ...               85.4586      344.4541         1.2932        1.3603   \n",
       "\n",
       "   IMF_SNR_SEO  IMF_SNR_TKEO  IMF_SNR_entropy  IMF_NSR_SEO  IMF_NSR_TKEO  \\\n",
       "0      51.6843       5.77840          23.2610      0.26850        5.8573   \n",
       "1      24.0230       6.79160          21.8851      0.26839        6.2366   \n",
       "2      60.0458      11.33760          41.7310      0.23034        4.8081   \n",
       "3      83.6201       5.07840          35.8179      0.23529        4.2629   \n",
       "4     102.9371       5.60220          31.5211      0.30603        6.3500   \n",
       "5      59.8518       4.80770          26.7902      0.27327        6.1189   \n",
       "6      24.2781       2.46830          11.1578      0.13043        6.3795   \n",
       "7       2.6477       0.66830           2.7385      0.12413       20.5870   \n",
       "8      14.4723       1.50850           7.0518      0.12068        5.8035   \n",
       "9       0.5149       0.14478           1.0844      0.13773       24.4549   \n",
       "\n",
       "   IMF_NSR_entropy  \n",
       "0          0.20023  \n",
       "1          0.20336  \n",
       "2          0.17296  \n",
       "3          0.17510  \n",
       "4          0.21877  \n",
       "5          0.19882  \n",
       "6          0.13555  \n",
       "7          0.13759  \n",
       "8          0.13288  \n",
       "9          0.13205  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,34:56]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,34:56]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.35      0.76      0.48        38\\n          1       0.87      0.52      0.65       112\\n\\navg / total       0.73      0.58      0.61       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.50      0.42      0.46        38\\n          1       0.81      0.86      0.83       112\\n\\navg / total       0.73      0.75      0.74       150\\n')\n",
      "('LR', '             precision    recall  f1-score   support\\n\\n          0       0.00      0.00      0.00        38\\n          1       0.75      1.00      0.85       112\\n\\navg / total       0.56      0.75      0.64       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.37      0.47      0.41        38\\n          1       0.80      0.72      0.76       112\\n\\navg / total       0.69      0.66      0.67       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.58)\n",
      "('Random Forest', 0.7466666666666667)\n",
      "('LR', 0.7466666666666667)\n",
      "('DT', 0.66)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_Log_energy</th>\n",
       "      <th>mean_MFCC_0th_coef</th>\n",
       "      <th>mean_MFCC_1st_coef</th>\n",
       "      <th>mean_MFCC_2nd_coef</th>\n",
       "      <th>mean_MFCC_3rd_coef</th>\n",
       "      <th>mean_MFCC_4th_coef</th>\n",
       "      <th>mean_MFCC_5th_coef</th>\n",
       "      <th>mean_MFCC_6th_coef</th>\n",
       "      <th>mean_MFCC_7th_coef</th>\n",
       "      <th>mean_MFCC_8th_coef</th>\n",
       "      <th>...</th>\n",
       "      <th>std_3rd_delta_delta</th>\n",
       "      <th>std_4th_delta_delta</th>\n",
       "      <th>std_5th_delta_delta</th>\n",
       "      <th>std_6th_delta_delta</th>\n",
       "      <th>std_7th_delta_delta</th>\n",
       "      <th>std_8th_delta_delta</th>\n",
       "      <th>std_9th_delta_delta</th>\n",
       "      <th>std_10th_delta_delta</th>\n",
       "      <th>std_11th_delta_delta</th>\n",
       "      <th>std_12th_delta_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.1817</td>\n",
       "      <td>11.4283</td>\n",
       "      <td>8.4781</td>\n",
       "      <td>2.487400</td>\n",
       "      <td>0.02292</td>\n",
       "      <td>0.55527</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-1.51440</td>\n",
       "      <td>0.39725</td>\n",
       "      <td>0.617750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021565</td>\n",
       "      <td>0.029594</td>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.012130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.6074</td>\n",
       "      <td>12.1387</td>\n",
       "      <td>8.5008</td>\n",
       "      <td>2.898600</td>\n",
       "      <td>-0.46733</td>\n",
       "      <td>0.45984</td>\n",
       "      <td>-1.34270</td>\n",
       "      <td>-1.67590</td>\n",
       "      <td>0.17695</td>\n",
       "      <td>0.729440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.038709</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.019374</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.016128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0512</td>\n",
       "      <td>11.3956</td>\n",
       "      <td>7.6362</td>\n",
       "      <td>3.220800</td>\n",
       "      <td>-0.48228</td>\n",
       "      <td>0.28918</td>\n",
       "      <td>-1.73340</td>\n",
       "      <td>-1.49290</td>\n",
       "      <td>0.70520</td>\n",
       "      <td>0.511730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024740</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>0.025675</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.024819</td>\n",
       "      <td>0.016553</td>\n",
       "      <td>0.023186</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.016257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.8453</td>\n",
       "      <td>13.7399</td>\n",
       "      <td>6.7365</td>\n",
       "      <td>3.102300</td>\n",
       "      <td>0.75419</td>\n",
       "      <td>-1.55900</td>\n",
       "      <td>-1.00830</td>\n",
       "      <td>-0.65556</td>\n",
       "      <td>0.86495</td>\n",
       "      <td>1.483700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.013770</td>\n",
       "      <td>0.015713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1542</td>\n",
       "      <td>14.7643</td>\n",
       "      <td>6.4634</td>\n",
       "      <td>2.945100</td>\n",
       "      <td>0.83210</td>\n",
       "      <td>-1.75500</td>\n",
       "      <td>-1.03920</td>\n",
       "      <td>-0.51876</td>\n",
       "      <td>0.94225</td>\n",
       "      <td>1.549800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.014478</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>0.013618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.8020</td>\n",
       "      <td>13.8952</td>\n",
       "      <td>6.7310</td>\n",
       "      <td>2.679900</td>\n",
       "      <td>0.92168</td>\n",
       "      <td>-1.75190</td>\n",
       "      <td>-0.85935</td>\n",
       "      <td>-0.62433</td>\n",
       "      <td>0.83613</td>\n",
       "      <td>1.567300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027266</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.017719</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.018090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.8604</td>\n",
       "      <td>16.8423</td>\n",
       "      <td>4.9305</td>\n",
       "      <td>0.747690</td>\n",
       "      <td>1.25620</td>\n",
       "      <td>-2.70220</td>\n",
       "      <td>-2.87990</td>\n",
       "      <td>-0.97391</td>\n",
       "      <td>1.36920</td>\n",
       "      <td>0.120460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.014555</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.017169</td>\n",
       "      <td>0.012914</td>\n",
       "      <td>0.014208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.9129</td>\n",
       "      <td>16.1057</td>\n",
       "      <td>6.8758</td>\n",
       "      <td>-0.664620</td>\n",
       "      <td>0.52793</td>\n",
       "      <td>-1.62730</td>\n",
       "      <td>-2.44040</td>\n",
       "      <td>-1.00650</td>\n",
       "      <td>0.41979</td>\n",
       "      <td>0.170390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>0.021660</td>\n",
       "      <td>0.014790</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.014510</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.009724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.9090</td>\n",
       "      <td>16.7174</td>\n",
       "      <td>6.4642</td>\n",
       "      <td>-1.378100</td>\n",
       "      <td>1.33950</td>\n",
       "      <td>-1.31610</td>\n",
       "      <td>-2.85260</td>\n",
       "      <td>-1.45340</td>\n",
       "      <td>0.76818</td>\n",
       "      <td>-0.013419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.023025</td>\n",
       "      <td>0.015222</td>\n",
       "      <td>0.020505</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.013336</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>0.009055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.4496</td>\n",
       "      <td>12.1136</td>\n",
       "      <td>6.7840</td>\n",
       "      <td>-0.034732</td>\n",
       "      <td>-1.91570</td>\n",
       "      <td>-1.77910</td>\n",
       "      <td>-1.67220</td>\n",
       "      <td>-1.87310</td>\n",
       "      <td>0.46320</td>\n",
       "      <td>0.928420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.019883</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.012960</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>0.013365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_Log_energy  mean_MFCC_0th_coef  mean_MFCC_1st_coef  \\\n",
       "0           9.1817             11.4283              8.4781   \n",
       "1           9.6074             12.1387              8.5008   \n",
       "2           9.0512             11.3956              7.6362   \n",
       "3           9.8453             13.7399              6.7365   \n",
       "4          10.1542             14.7643              6.4634   \n",
       "5           9.8020             13.8952              6.7310   \n",
       "6           9.8604             16.8423              4.9305   \n",
       "7           9.9129             16.1057              6.8758   \n",
       "8           9.9090             16.7174              6.4642   \n",
       "9           9.4496             12.1136              6.7840   \n",
       "\n",
       "   mean_MFCC_2nd_coef  mean_MFCC_3rd_coef  mean_MFCC_4th_coef  \\\n",
       "0            2.487400             0.02292             0.55527   \n",
       "1            2.898600            -0.46733             0.45984   \n",
       "2            3.220800            -0.48228             0.28918   \n",
       "3            3.102300             0.75419            -1.55900   \n",
       "4            2.945100             0.83210            -1.75500   \n",
       "5            2.679900             0.92168            -1.75190   \n",
       "6            0.747690             1.25620            -2.70220   \n",
       "7           -0.664620             0.52793            -1.62730   \n",
       "8           -1.378100             1.33950            -1.31610   \n",
       "9           -0.034732            -1.91570            -1.77910   \n",
       "\n",
       "   mean_MFCC_5th_coef  mean_MFCC_6th_coef  mean_MFCC_7th_coef  \\\n",
       "0            -1.63340            -1.51440             0.39725   \n",
       "1            -1.34270            -1.67590             0.17695   \n",
       "2            -1.73340            -1.49290             0.70520   \n",
       "3            -1.00830            -0.65556             0.86495   \n",
       "4            -1.03920            -0.51876             0.94225   \n",
       "5            -0.85935            -0.62433             0.83613   \n",
       "6            -2.87990            -0.97391             1.36920   \n",
       "7            -2.44040            -1.00650             0.41979   \n",
       "8            -2.85260            -1.45340             0.76818   \n",
       "9            -1.67220            -1.87310             0.46320   \n",
       "\n",
       "   mean_MFCC_8th_coef          ...           std_3rd_delta_delta  \\\n",
       "0            0.617750          ...                      0.021565   \n",
       "1            0.729440          ...                      0.024058   \n",
       "2            0.511730          ...                      0.024740   \n",
       "3            1.483700          ...                      0.019442   \n",
       "4            1.549800          ...                      0.022486   \n",
       "5            1.567300          ...                      0.027266   \n",
       "6            0.120460          ...                      0.024326   \n",
       "7            0.170390          ...                      0.020663   \n",
       "8           -0.013419          ...                      0.025620   \n",
       "9            0.928420          ...                      0.018277   \n",
       "\n",
       "   std_4th_delta_delta  std_5th_delta_delta  std_6th_delta_delta  \\\n",
       "0             0.029594             0.015351             0.014642   \n",
       "1             0.038709             0.012191             0.025500   \n",
       "2             0.039222             0.025675             0.024607   \n",
       "3             0.018382             0.018555             0.017274   \n",
       "4             0.020515             0.017161             0.013791   \n",
       "5             0.024693             0.015578             0.018248   \n",
       "6             0.019952             0.017329             0.017874   \n",
       "7             0.021660             0.014790             0.014798   \n",
       "8             0.023025             0.015222             0.020505   \n",
       "9             0.013201             0.014997             0.019883   \n",
       "\n",
       "   std_7th_delta_delta  std_8th_delta_delta  std_9th_delta_delta  \\\n",
       "0             0.019681             0.012829             0.021703   \n",
       "1             0.019374             0.010645             0.020296   \n",
       "2             0.024819             0.016553             0.023186   \n",
       "3             0.016129             0.015533             0.011976   \n",
       "4             0.016059             0.014382             0.012686   \n",
       "5             0.018580             0.017719             0.015025   \n",
       "6             0.019139             0.014555             0.013068   \n",
       "7             0.015257             0.013645             0.014510   \n",
       "8             0.019041             0.015607             0.013336   \n",
       "9             0.014610             0.017956             0.012766   \n",
       "\n",
       "   std_10th_delta_delta  std_11th_delta_delta  std_12th_delta_delta  \n",
       "0              0.017089              0.010043              0.012130  \n",
       "1              0.015389              0.015488              0.016128  \n",
       "2              0.017217              0.015073              0.016257  \n",
       "3              0.015529              0.013770              0.015713  \n",
       "4              0.014478              0.013402              0.013618  \n",
       "5              0.019438              0.017700              0.018090  \n",
       "6              0.017169              0.012914              0.014208  \n",
       "7              0.012634              0.010968              0.009724  \n",
       "8              0.010832              0.010263              0.009055  \n",
       "9              0.012960              0.011516              0.013365  \n",
       "\n",
       "[10 rows x 84 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,56:140]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,56:140]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.5466666666666666)\n",
      "('Random Forest', 0.6866666666666666)\n",
      "('LR', 0.7466666666666667)\n",
      "('DT', 0.68)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.35      0.89      0.50        38\\n          1       0.92      0.43      0.59       112\\n\\navg / total       0.78      0.55      0.56       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.40      0.45      0.42        38\\n          1       0.80      0.77      0.79       112\\n\\navg / total       0.70      0.69      0.69       150\\n')\n",
      "('LR', '             precision    recall  f1-score   support\\n\\n          0       0.50      0.42      0.46        38\\n          1       0.81      0.86      0.83       112\\n\\navg / total       0.73      0.75      0.74       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.40      0.50      0.44        38\\n          1       0.81      0.74      0.78       112\\n\\navg / total       0.71      0.68      0.69       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ea</th>\n",
       "      <th>Ed_1_coef</th>\n",
       "      <th>Ed_2_coef</th>\n",
       "      <th>Ed_3_coef</th>\n",
       "      <th>Ed_4_coef</th>\n",
       "      <th>Ed_5_coef</th>\n",
       "      <th>Ed_6_coef</th>\n",
       "      <th>Ed_7_coef</th>\n",
       "      <th>Ed_8_coef</th>\n",
       "      <th>Ed_9_coef</th>\n",
       "      <th>...</th>\n",
       "      <th>app_LT_TKEO_std_1_coef</th>\n",
       "      <th>app_LT_TKEO_std_2_coef</th>\n",
       "      <th>app_LT_TKEO_std_3_coef</th>\n",
       "      <th>app_LT_TKEO_std_4_coef</th>\n",
       "      <th>app_LT_TKEO_std_5_coef</th>\n",
       "      <th>app_LT_TKEO_std_6_coef</th>\n",
       "      <th>app_LT_TKEO_std_7_coef</th>\n",
       "      <th>app_LT_TKEO_std_8_coef</th>\n",
       "      <th>app_LT_TKEO_std_9_coef</th>\n",
       "      <th>app_LT_TKEO_std_10_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.9996</td>\n",
       "      <td>1.530000e-07</td>\n",
       "      <td>8.860000e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2990</td>\n",
       "      <td>16.7003</td>\n",
       "      <td>42.0762</td>\n",
       "      <td>101.0889</td>\n",
       "      <td>228.8489</td>\n",
       "      <td>493.8563</td>\n",
       "      <td>1015.7707</td>\n",
       "      <td>2091.9460</td>\n",
       "      <td>4188.2456</td>\n",
       "      <td>8373.9278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.9998</td>\n",
       "      <td>1.680000e-07</td>\n",
       "      <td>1.230000e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2381</td>\n",
       "      <td>16.5376</td>\n",
       "      <td>41.7306</td>\n",
       "      <td>100.0918</td>\n",
       "      <td>226.9019</td>\n",
       "      <td>489.9169</td>\n",
       "      <td>1006.3702</td>\n",
       "      <td>2074.4541</td>\n",
       "      <td>4148.9889</td>\n",
       "      <td>8298.1606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99.9999</td>\n",
       "      <td>1.170000e-07</td>\n",
       "      <td>9.450000e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2163</td>\n",
       "      <td>16.4817</td>\n",
       "      <td>41.4869</td>\n",
       "      <td>99.6154</td>\n",
       "      <td>225.7803</td>\n",
       "      <td>486.9865</td>\n",
       "      <td>1001.7348</td>\n",
       "      <td>2064.1067</td>\n",
       "      <td>4127.0967</td>\n",
       "      <td>8254.7868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.0823</td>\n",
       "      <td>8.545600e-03</td>\n",
       "      <td>2.058600e-02</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.041488</td>\n",
       "      <td>0.062341</td>\n",
       "      <td>0.090279</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.234650</td>\n",
       "      <td>0.067636</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7833</td>\n",
       "      <td>16.8216</td>\n",
       "      <td>41.3157</td>\n",
       "      <td>94.4579</td>\n",
       "      <td>211.1565</td>\n",
       "      <td>443.3447</td>\n",
       "      <td>955.8128</td>\n",
       "      <td>1890.1299</td>\n",
       "      <td>3910.7029</td>\n",
       "      <td>7698.9389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.6930</td>\n",
       "      <td>7.861100e-03</td>\n",
       "      <td>1.123800e-02</td>\n",
       "      <td>0.014564</td>\n",
       "      <td>0.055365</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.057492</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.123360</td>\n",
       "      <td>0.101750</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9366</td>\n",
       "      <td>18.3595</td>\n",
       "      <td>46.2704</td>\n",
       "      <td>108.6792</td>\n",
       "      <td>244.0607</td>\n",
       "      <td>541.2414</td>\n",
       "      <td>1057.2566</td>\n",
       "      <td>2242.5460</td>\n",
       "      <td>4297.4639</td>\n",
       "      <td>8645.2845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99.5213</td>\n",
       "      <td>1.149600e-02</td>\n",
       "      <td>1.640800e-02</td>\n",
       "      <td>0.026368</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.052605</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.061452</td>\n",
       "      <td>0.132370</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3764</td>\n",
       "      <td>22.7007</td>\n",
       "      <td>55.3092</td>\n",
       "      <td>118.2894</td>\n",
       "      <td>260.5987</td>\n",
       "      <td>571.4985</td>\n",
       "      <td>1090.6415</td>\n",
       "      <td>2280.6768</td>\n",
       "      <td>4490.8117</td>\n",
       "      <td>9009.3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.9936</td>\n",
       "      <td>5.140000e-07</td>\n",
       "      <td>7.200000e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1081</td>\n",
       "      <td>18.8481</td>\n",
       "      <td>47.4972</td>\n",
       "      <td>113.9745</td>\n",
       "      <td>258.2175</td>\n",
       "      <td>557.1802</td>\n",
       "      <td>1147.0422</td>\n",
       "      <td>2358.5320</td>\n",
       "      <td>4737.9142</td>\n",
       "      <td>9465.1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>99.9954</td>\n",
       "      <td>4.880000e-07</td>\n",
       "      <td>5.600000e-06</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0290</td>\n",
       "      <td>18.6623</td>\n",
       "      <td>46.9588</td>\n",
       "      <td>112.5594</td>\n",
       "      <td>255.4390</td>\n",
       "      <td>550.5283</td>\n",
       "      <td>1137.2850</td>\n",
       "      <td>2336.1915</td>\n",
       "      <td>4689.1889</td>\n",
       "      <td>9368.8262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99.9990</td>\n",
       "      <td>3.160000e-07</td>\n",
       "      <td>3.410000e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1013</td>\n",
       "      <td>18.8287</td>\n",
       "      <td>47.4020</td>\n",
       "      <td>113.8672</td>\n",
       "      <td>258.0441</td>\n",
       "      <td>556.8407</td>\n",
       "      <td>1144.0632</td>\n",
       "      <td>2359.9277</td>\n",
       "      <td>4711.4346</td>\n",
       "      <td>9428.5914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>99.9991</td>\n",
       "      <td>4.690000e-07</td>\n",
       "      <td>2.170000e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2956</td>\n",
       "      <td>21.9842</td>\n",
       "      <td>55.5210</td>\n",
       "      <td>133.5242</td>\n",
       "      <td>302.3896</td>\n",
       "      <td>652.3802</td>\n",
       "      <td>1343.0375</td>\n",
       "      <td>2767.5847</td>\n",
       "      <td>5528.7612</td>\n",
       "      <td>11060.3011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ea     Ed_1_coef     Ed_2_coef  Ed_3_coef  Ed_4_coef  Ed_5_coef  \\\n",
       "0  99.9996  1.530000e-07  8.860000e-07   0.000007   0.000021   0.000102   \n",
       "1  99.9998  1.680000e-07  1.230000e-06   0.000012   0.000031   0.000046   \n",
       "2  99.9999  1.170000e-07  9.450000e-07   0.000010   0.000034   0.000024   \n",
       "3  99.0823  8.545600e-03  2.058600e-02   0.025449   0.041488   0.062341   \n",
       "4  98.6930  7.861100e-03  1.123800e-02   0.014564   0.055365   0.030612   \n",
       "5  99.5213  1.149600e-02  1.640800e-02   0.026368   0.110000   0.052605   \n",
       "6  99.9936  5.140000e-07  7.200000e-06   0.000015   0.000165   0.000061   \n",
       "7  99.9954  4.880000e-07  5.600000e-06   0.000047   0.000461   0.000193   \n",
       "8  99.9990  3.160000e-07  3.410000e-06   0.000022   0.000073   0.000037   \n",
       "9  99.9991  4.690000e-07  2.170000e-06   0.000004   0.000126   0.000048   \n",
       "\n",
       "   Ed_6_coef  Ed_7_coef  Ed_8_coef  Ed_9_coef           ...             \\\n",
       "0   0.000013   0.000041   0.000027   0.000010           ...              \n",
       "1   0.000036   0.000005   0.000074   0.000009           ...              \n",
       "2   0.000035   0.000002   0.000001   0.000001           ...              \n",
       "3   0.090279   0.133310   0.234650   0.067636           ...              \n",
       "4   0.057492   0.113500   0.123360   0.101750           ...              \n",
       "5   0.007210   0.061452   0.132370   0.004154           ...              \n",
       "6   0.000039   0.000027   0.000348   0.000444           ...              \n",
       "7   0.000076   0.000123   0.000832   0.000124           ...              \n",
       "8   0.000026   0.000040   0.000160   0.000056           ...              \n",
       "9   0.000035   0.000019   0.000076   0.000071           ...              \n",
       "\n",
       "   app_LT_TKEO_std_1_coef  app_LT_TKEO_std_2_coef  app_LT_TKEO_std_3_coef  \\\n",
       "0                  6.2990                 16.7003                 42.0762   \n",
       "1                  6.2381                 16.5376                 41.7306   \n",
       "2                  6.2163                 16.4817                 41.4869   \n",
       "3                  6.7833                 16.8216                 41.3157   \n",
       "4                  6.9366                 18.3595                 46.2704   \n",
       "5                  8.3764                 22.7007                 55.3092   \n",
       "6                  7.1081                 18.8481                 47.4972   \n",
       "7                  7.0290                 18.6623                 46.9588   \n",
       "8                  7.1013                 18.8287                 47.4020   \n",
       "9                  8.2956                 21.9842                 55.5210   \n",
       "\n",
       "   app_LT_TKEO_std_4_coef  app_LT_TKEO_std_5_coef  app_LT_TKEO_std_6_coef  \\\n",
       "0                101.0889                228.8489                493.8563   \n",
       "1                100.0918                226.9019                489.9169   \n",
       "2                 99.6154                225.7803                486.9865   \n",
       "3                 94.4579                211.1565                443.3447   \n",
       "4                108.6792                244.0607                541.2414   \n",
       "5                118.2894                260.5987                571.4985   \n",
       "6                113.9745                258.2175                557.1802   \n",
       "7                112.5594                255.4390                550.5283   \n",
       "8                113.8672                258.0441                556.8407   \n",
       "9                133.5242                302.3896                652.3802   \n",
       "\n",
       "   app_LT_TKEO_std_7_coef  app_LT_TKEO_std_8_coef  app_LT_TKEO_std_9_coef  \\\n",
       "0               1015.7707               2091.9460               4188.2456   \n",
       "1               1006.3702               2074.4541               4148.9889   \n",
       "2               1001.7348               2064.1067               4127.0967   \n",
       "3                955.8128               1890.1299               3910.7029   \n",
       "4               1057.2566               2242.5460               4297.4639   \n",
       "5               1090.6415               2280.6768               4490.8117   \n",
       "6               1147.0422               2358.5320               4737.9142   \n",
       "7               1137.2850               2336.1915               4689.1889   \n",
       "8               1144.0632               2359.9277               4711.4346   \n",
       "9               1343.0375               2767.5847               5528.7612   \n",
       "\n",
       "   app_LT_TKEO_std_10_coef  \n",
       "0                8373.9278  \n",
       "1                8298.1606  \n",
       "2                8254.7868  \n",
       "3                7698.9389  \n",
       "4                8645.2845  \n",
       "5                9009.3152  \n",
       "6                9465.1464  \n",
       "7                9368.8262  \n",
       "8                9428.5914  \n",
       "9               11060.3011  \n",
       "\n",
       "[10 rows x 182 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,140:322]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,140:322]\n",
    "y= df['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.7133333333333334)\n",
      "('Random Forest', 0.6666666666666666)\n",
      "('LR', 0.74)\n",
      "('DT', 0.6733333333333333)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.35      0.16      0.22        38\\n          1       0.76      0.90      0.82       112\\n\\navg / total       0.66      0.71      0.67       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.33      0.32      0.32        38\\n          1       0.77      0.79      0.78       112\\n\\navg / total       0.66      0.67      0.66       150\\n')\n",
      "('LR', '             precision    recall  f1-score   support\\n\\n          0       0.40      0.05      0.09        38\\n          1       0.75      0.97      0.85       112\\n\\navg / total       0.66      0.74      0.66       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.38      0.45      0.41        38\\n          1       0.80      0.75      0.77       112\\n\\navg / total       0.69      0.67      0.68       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tqwt_energy_dec_1</th>\n",
       "      <th>tqwt_energy_dec_2</th>\n",
       "      <th>tqwt_energy_dec_3</th>\n",
       "      <th>tqwt_energy_dec_4</th>\n",
       "      <th>tqwt_energy_dec_5</th>\n",
       "      <th>tqwt_energy_dec_6</th>\n",
       "      <th>tqwt_energy_dec_7</th>\n",
       "      <th>tqwt_energy_dec_8</th>\n",
       "      <th>tqwt_energy_dec_9</th>\n",
       "      <th>tqwt_energy_dec_10</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5466</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5530</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5399</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9761</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8832</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2888</td>\n",
       "      <td>4.8025</td>\n",
       "      <td>5.0734</td>\n",
       "      <td>7.0166</td>\n",
       "      <td>5.9966</td>\n",
       "      <td>5.2065</td>\n",
       "      <td>7.4246</td>\n",
       "      <td>3.4153</td>\n",
       "      <td>3.5046</td>\n",
       "      <td>3.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5541</td>\n",
       "      <td>117.2678</td>\n",
       "      <td>75.3156</td>\n",
       "      <td>32.0478</td>\n",
       "      <td>7.7060</td>\n",
       "      <td>3.1060</td>\n",
       "      <td>4.6206</td>\n",
       "      <td>12.8353</td>\n",
       "      <td>13.8300</td>\n",
       "      <td>7.7693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5800</td>\n",
       "      <td>3.8564</td>\n",
       "      <td>11.8909</td>\n",
       "      <td>7.2891</td>\n",
       "      <td>4.3682</td>\n",
       "      <td>3.6443</td>\n",
       "      <td>5.9610</td>\n",
       "      <td>11.7552</td>\n",
       "      <td>18.0927</td>\n",
       "      <td>5.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6118</td>\n",
       "      <td>2.2640</td>\n",
       "      <td>6.3993</td>\n",
       "      <td>4.4165</td>\n",
       "      <td>4.2662</td>\n",
       "      <td>3.6357</td>\n",
       "      <td>3.7346</td>\n",
       "      <td>2.9394</td>\n",
       "      <td>3.6216</td>\n",
       "      <td>3.8430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6796</td>\n",
       "      <td>1.6796</td>\n",
       "      <td>2.0474</td>\n",
       "      <td>2.8117</td>\n",
       "      <td>3.5070</td>\n",
       "      <td>3.2727</td>\n",
       "      <td>3.8415</td>\n",
       "      <td>3.9439</td>\n",
       "      <td>5.8807</td>\n",
       "      <td>38.7211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tqwt_energy_dec_1  tqwt_energy_dec_2  tqwt_energy_dec_3  tqwt_energy_dec_4  \\\n",
       "0           0.000011           0.000004           0.000005           0.000004   \n",
       "1           0.000028           0.000010           0.000011           0.000010   \n",
       "2           0.000034           0.000010           0.000008           0.000005   \n",
       "3           0.000051           0.000041           0.000041           0.000047   \n",
       "4           0.000044           0.000050           0.000061           0.000052   \n",
       "5           0.000040           0.000052           0.000053           0.000052   \n",
       "6           0.000080           0.000098           0.000292           0.000830   \n",
       "7           0.000103           0.000072           0.000117           0.000253   \n",
       "8           0.000044           0.000063           0.000096           0.000131   \n",
       "9           0.000014           0.000018           0.000023           0.000025   \n",
       "\n",
       "   tqwt_energy_dec_5  tqwt_energy_dec_6  tqwt_energy_dec_7  tqwt_energy_dec_8  \\\n",
       "0           0.000007           0.000039           0.000164           0.000376   \n",
       "1           0.000012           0.000025           0.000092           0.000276   \n",
       "2           0.000007           0.000036           0.000153           0.000352   \n",
       "3           0.000061           0.000042           0.000068           0.000132   \n",
       "4           0.000062           0.000043           0.000055           0.000136   \n",
       "5           0.000066           0.000051           0.000067           0.000211   \n",
       "6           0.001073           0.000807           0.001973           0.003858   \n",
       "7           0.000321           0.000256           0.000747           0.001827   \n",
       "8           0.000174           0.000424           0.001104           0.002425   \n",
       "9           0.000023           0.000064           0.000317           0.000787   \n",
       "\n",
       "   tqwt_energy_dec_9  tqwt_energy_dec_10            ...              \\\n",
       "0           0.000392            0.000406            ...               \n",
       "1           0.000404            0.000449            ...               \n",
       "2           0.000312            0.000320            ...               \n",
       "3           0.000167            0.000107            ...               \n",
       "4           0.000207            0.000107            ...               \n",
       "5           0.000327            0.000158            ...               \n",
       "6           0.003563            0.000786            ...               \n",
       "7           0.002080            0.000602            ...               \n",
       "8           0.002950            0.000758            ...               \n",
       "9           0.000901            0.000478            ...               \n",
       "\n",
       "   tqwt_kurtosisValue_dec_27  tqwt_kurtosisValue_dec_28  \\\n",
       "0                     1.5466                     1.5620   \n",
       "1                     1.5530                     1.5589   \n",
       "2                     1.5399                     1.5643   \n",
       "3                     6.9761                     3.7805   \n",
       "4                     7.8832                     6.1727   \n",
       "5                     6.2888                     4.8025   \n",
       "6                     1.5541                   117.2678   \n",
       "7                     1.5800                     3.8564   \n",
       "8                     1.6118                     2.2640   \n",
       "9                     2.6796                     1.6796   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_29  tqwt_kurtosisValue_dec_30  \\\n",
       "0                     2.6445                     3.8686   \n",
       "1                     3.6107                    23.5155   \n",
       "2                     2.3308                     9.4959   \n",
       "3                     3.5664                     5.2558   \n",
       "4                     5.8416                     6.0805   \n",
       "5                     5.0734                     7.0166   \n",
       "6                    75.3156                    32.0478   \n",
       "7                    11.8909                     7.2891   \n",
       "8                     6.3993                     4.4165   \n",
       "9                     2.0474                     2.8117   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_31  tqwt_kurtosisValue_dec_32  \\\n",
       "0                     4.2105                     5.1221   \n",
       "1                    14.1962                    11.0261   \n",
       "2                    10.7458                    11.0177   \n",
       "3                    14.0403                     4.2235   \n",
       "4                     5.7621                     7.7817   \n",
       "5                     5.9966                     5.2065   \n",
       "6                     7.7060                     3.1060   \n",
       "7                     4.3682                     3.6443   \n",
       "8                     4.2662                     3.6357   \n",
       "9                     3.5070                     3.2727   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_33  tqwt_kurtosisValue_dec_34  \\\n",
       "0                     4.4625                     2.6202   \n",
       "1                     9.5082                     6.5245   \n",
       "2                     4.8066                     2.9199   \n",
       "3                     4.6857                     4.8460   \n",
       "4                    11.6891                     8.2103   \n",
       "5                     7.4246                     3.4153   \n",
       "6                     4.6206                    12.8353   \n",
       "7                     5.9610                    11.7552   \n",
       "8                     3.7346                     2.9394   \n",
       "9                     3.8415                     3.9439   \n",
       "\n",
       "   tqwt_kurtosisValue_dec_35  tqwt_kurtosisValue_dec_36  \n",
       "0                     3.0004                    18.9405  \n",
       "1                     6.3431                    45.1780  \n",
       "2                     3.1495                     4.7666  \n",
       "3                     6.2650                     4.0603  \n",
       "4                     5.0559                     6.1164  \n",
       "5                     3.5046                     3.2250  \n",
       "6                    13.8300                     7.7693  \n",
       "7                    18.0927                     5.0448  \n",
       "8                     3.6216                     3.8430  \n",
       "9                     5.8807                    38.7211  \n",
       "\n",
       "[10 rows x 432 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.iloc[:,322:754]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,322:754]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7933333333333333"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.5133333333333333)\n",
      "('Random Forest', 0.8066666666666666)\n",
      "('LR', 0.8)\n",
      "('DT', 0.76)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', '             precision    recall  f1-score   support\\n\\n          0       0.31      0.74      0.43        38\\n          1       0.83      0.44      0.57       112\\n\\navg / total       0.70      0.51      0.54       150\\n')\n",
      "('Random Forest', '             precision    recall  f1-score   support\\n\\n          0       0.66      0.50      0.57        38\\n          1       0.84      0.91      0.88       112\\n\\navg / total       0.80      0.81      0.80       150\\n')\n",
      "('LR', '             precision    recall  f1-score   support\\n\\n          0       0.62      0.55      0.58        38\\n          1       0.85      0.88      0.87       112\\n\\navg / total       0.79      0.80      0.80       150\\n')\n",
      "('DT', '             precision    recall  f1-score   support\\n\\n          0       0.53      0.53      0.53        38\\n          1       0.84      0.84      0.84       112\\n\\navg / total       0.76      0.76      0.76       150\\n')\n"
     ]
    }
   ],
   "source": [
    "for entry in classificationreport:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,2:754]\n",
    "y= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    #print(\"Train:\", train_index, \"Validation:\", test_index) \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classificationreport = []\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Gaussian NB',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Gaussian NB',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('Random Forest',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('Random Forest',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('LR',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('LR',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "testac = accuracy_score(y_test, preds)\n",
    "classifiers.append(('DT',testac))\n",
    "from sklearn.metrics import classification_report\n",
    "classreport = classification_report(y_test, preds)\n",
    "classificationreport.append(('DT',classreport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = GaussianNB()\n",
    "model2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "model3 = LogisticRegression()\n",
    "model4 = DecisionTreeClassifier(random_state=0)\n",
    "model = VotingClassifier(estimators=[('GNB', model1), ('rf', model2), ('LR',model3),('DT',model4)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gaussian NB', 0.7133333333333334)\n",
      "('Random Forest', 0.78)\n",
      "('LR', 0.7333333333333333)\n",
      "('DT', 0.7266666666666667)\n"
     ]
    }
   ],
   "source": [
    "for entry in classifiers:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection using recursive feature extraction, RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,2:23]\n",
    "Y = df['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 10\n",
      "Selected Features: [False  True  True  True  True False False False False False False False\n",
      "  True  True  True False  True  True False  True False]\n",
      "Feature Ranking: [ 7  1  1  1  1  9 11  4 12 10  8  6  1  1  1  2  1  1  3  1  5]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 10)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,23:26]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 2\n",
      "Selected Features: [False  True  True]\n",
      "Feature Ranking: [2 1 1]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 2)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,26:34]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 5\n",
      "Selected Features: [ True  True  True  True False  True False False]\n",
      "Feature Ranking: [1 1 1 1 2 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 5)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,34:56]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 15\n",
      "Selected Features: [ True  True  True False False False  True  True  True False False  True\n",
      "  True  True  True  True  True  True  True False  True False]\n",
      "Feature Ranking: [1 1 1 2 4 6 1 1 1 7 8 1 1 1 1 1 1 1 1 3 1 5]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 15)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,56:140]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 20\n",
      "Selected Features: [False False False  True False False False False  True False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False  True  True False False\n",
      " False  True  True  True False False False  True  True False  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False]\n",
      "Feature Ranking: [ 8 39 47  1  6 40  4  2  1 38 29  1 28 36 30 18 37 16 41 44 58 59 45 46\n",
      " 48 43 42 53 61 54 49 60 50 62 56 64 57 63 65 52 51 55  1 25  1  1  5 17\n",
      " 20  1  1  1 34 26 23  1  1  9  1  1  1  1  1  1  1  1  1 10 12 31  3 21\n",
      "  7 13 22 15 24 11 14 27 19 32 33 35]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 20)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,140:322]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 30\n",
      "Selected Features: [False False False False False False False False False False False  True\n",
      "  True  True False  True  True  True  True  True  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n",
      "Feature Ranking: [ 76 137 143 141 138 135 140 131 132 134 121   1   1   1  20   1   1   1\n",
      "   1   1   1  77  60  59  63  65  74  88  84  91  81  78  42  28  83  26\n",
      "  24  15  23  30   5  41  49  36  31  17  12  11  18  19   2   1   1   1\n",
      "   1   1   1   1   1   1   1  33  40  43  48  52  56  57  58  54  53  47\n",
      "  92  80  22  21  16  39   6   4   1  82  55  13   9   8   1   1  35  51\n",
      "   1  75 151 153 152 150 146 149 145 148 147 136 114 110 106 104 113 112\n",
      "  97 107 108  64  37  79  87 103  98 109  89  93  86  94 139 144 142 125\n",
      " 124 127 117 130 122 105 129 133 126 123 118 119 115 128 120 101  10   7\n",
      "   3   1   1   1   1   1   1   1  45  50  61  66  70  73  72  71  69  67\n",
      " 116 111 100  96  90  68  46  38  32  27 102  99  95  85  62  44  34  29\n",
      "  25  14]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 30)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,322:754]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 15\n",
      "Selected Features: [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False  True\n",
      "  True False False False False  True  True  True  True  True  True False\n",
      "  True False False  True False False False False  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False]\n",
      "Feature Ranking: [352 343 319 335 289 286 302 294 293 276 272 280 349 271 216 275 180 242\n",
      " 193 279 211 265 176 207 178 183 200 277 307 311 290 274 264 288 308 273\n",
      "  78  81  64  66  58  50  52  53  60  47  45  54  28  33  40  59  25  38\n",
      "  29  43  32  44  68  26  24  35  18  42  51  70  73  76  84  69  72  65\n",
      "  17   8   7   2   1  23  31  19  10  49   9   1   1   5   4  13  39   1\n",
      "   1   1   1   1   1  16   1   3   6   1  30  21  15  11   1   1   1  12\n",
      " 373 379 367 361 356 342 351 340 355 331 315 320 234 227 248 237 130 196\n",
      " 375 197 151 199 129 118 148 121  96 181 228 300 226 182 170 225 232 203\n",
      " 368 371 358 353 334 318 338 332 339 323 305 327 270 250 316 239 154 161\n",
      " 245 172 192 221 185 159 166 171 128 187 257 278 230 190 184 224 235 204\n",
      " 382 381 383 380 377 378 374 376 365 359 372 369 362 341 366 354 346 364\n",
      " 326 347 336 329 357 344 285 313 333 299 283 348 322 328 325 363 330 297\n",
      " 418 413 416 414 410 417 406 415 411 407 397 400 401 395 403 392 398 390\n",
      " 386 402 396 385 384 391 387 404 394 409 393 399 412 408 389 388 405 295\n",
      " 360 337 310 314 317 291 298 303 321 282 269 324 236 231 262 229 167 209\n",
      " 268 217 158 219 141 165 127 116 132 195 252 240 214 189 168 174 186 169\n",
      " 296 301 260 253 243 218 241 247 258 213 198 202 345 208 201 139 107 115\n",
      " 146 123 119 144 135 150 108  99 113 157 259 220 177 152 131 136 145 124\n",
      " 249 287 246 255 238 215 233 254 244 212 191 205 251 210 194 140 110 114\n",
      " 142 122 120 147 134 156 106 101 112 143 222 223 179 153 133 137 149 125\n",
      "  79  92 175 105 261  98 155 267 188 173 164 263 304 370 284 281 256 350\n",
      " 266 306 292 312 309 162 163 160  86  83  87 206  97  95  94 109 100  85\n",
      "   1  14  20  22  27  34  61  71  77  74  67  88 138 126 102 103  91  89\n",
      "  93  90 104 117 111  80  48  37  36  46  62  75  82  63  57  55  56  41]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 15)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "X = array[:,2:754]\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 97\n",
      "Selected Features: [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False  True  True  True  True False  True False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False  True False  True False\n",
      "  True  True  True False False  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Feature Ranking: [354 219 211 143 160 507 549 489 594 523 509 490 347 187 407 384 340 321\n",
      " 262 320 140 147 111 131 121  77  12   1  30  79  31  21 303  83 124 229\n",
      " 249 421   1 207 183 519 548 142  35 167 221 242  34 190 109 313 138 355\n",
      " 158 350 315 137 154 227 178 174 180 293 316 186 263 271 473 456 500 480\n",
      " 590 545 533 603 563 575 546 555 560 576 577 564 567 619 609 570 574 610\n",
      " 588 612 579 580 578 584 189 173 199 264 226 215 223 212 225 230 251 247\n",
      " 248 273 331 301 357 383 366 362 376 372 378 386 389 391 398 406 411 387\n",
      " 415 439 416 414 429 424 428 436 437 440 446 449  92 513 517 477 525 495\n",
      " 542 463 485 478 360   1   1   1   1   1   1   1   1   1   1 129  51  53\n",
      "  81  90  96 106 105 112 101  64  27   5 132   1   1   1   1  17   1  41\n",
      "  28  25  22   1   1   1   1   1   1   1   1   1  46   1   1   1   1   1\n",
      "   1  26  37  45  55  65  68  69  70  67  66  33 139  78   1  11   1   7\n",
      "   1   1   1  18  80   1   1   1   1   1   1   1   1  91 589 613 599 593\n",
      " 566 591 552 614 571 498 214 181 177 164 195 185 135 146 182  87  47  76\n",
      " 159 404 243 279  97 103  94 115 531 597 482 443 408 455 283 367 379 175\n",
      " 460 479 445 388 284 307 213 499 317 155   1   1   1   1   1   1   1   1\n",
      "   1   1  49  59  71  73  84  88  86  85  82  74 257 188 156 127 104  75\n",
      "  56  36  23   6 169 145 113  95  72  48  32  20   1   1 524 527 514 538\n",
      " 492 487 497 481 475 466 458 431 371 349 322 308 393 441 484 396 353 352\n",
      " 397 351 288 289 298 370 447 554 505 468 461 491 512 467 166 161 141 125\n",
      "  89  62  60  42  40  38  29  14   2   1   1   1   1  10  19  24 128 130\n",
      "  57  43  63 117  15  61  44 122 136 118 133 123 126 120   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   9   1   1   1   1   1   1   1   1 607 605 600 618\n",
      " 558 551 553 535 530 518 502 465 412 361 323 275 302 274 325 405 394 451\n",
      " 285 234 417 252 184 245 280 457 409 314 294 402 427 381 587 592 595 601\n",
      " 536 522 539 521 515 506 488 462 413 365 338 312 339 392 369 444 422 401\n",
      " 425 335 373 319 241 300 368 483 410 324 328 400 426 374 617 616 620 621\n",
      " 608 611 615 606 596 581 604 598 582 568 569 586 585 583 559 562 572 540\n",
      " 573 550 501 529 526 503 496 561 532 537 541 543 547 452 656 652 655 653\n",
      " 648 654 645 649 650 646 639 642 644 643 641 631 634 630 624 636 637 626\n",
      " 623 638 629 632 625 628 633 640 635 651 622 627 647 454 602 556 557 511\n",
      " 469 459 453 442 434 419 395 346 299 281 258 237 246 261 297 337 486 520\n",
      " 295 272 403 239 224 265 377 464 382 333 306 330 344 309 390 433 448 420\n",
      " 359 334 345 326 311 286 268 232 209 206 203 201 220 256 277 418 329 348\n",
      " 278 240 292 202 197 222 304 432 305 260 235 250 259 228 385 435 450 423\n",
      " 356 332 343 327 310 290 269 233 210 204 205 200 217 255 282 438 336 341\n",
      " 276 244 287 208 196 216 296 430 318 267 236 253 266 218 165 291 399 270\n",
      " 472 254 364 470 380 375 358 504 494 493 528 508 476 544 474 565 534 516\n",
      " 510 363 471 238 179 162 176 342 194 191 193 231 198 170  16   8   4   3\n",
      "   1  13  39  93 102 107 114 110 144 152 151 148 149 150 157 153 163 171\n",
      " 168 192 116  54  50  58  98 134 172 119 108 100  99  52]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 97)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
